{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Models\n",
    "\n",
    "**Group 19**\n",
    "\n",
    "*Cátia Antunes* (fc60494) - 5h  \n",
    "*Donato Aveiro* (fc46269) -  \n",
    "*Márcia Vital* (fc59488) -   \n",
    "*Seán Gorman* (fc59492) -  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first home assignment was to predict the critical temperature of a superconductor based on 81 extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of all modules required for the assignment\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading \n",
    "df1 = pd.read_csv(\"train.csv\")\n",
    "#df1.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began our data preprocess by taking a look at the data types of each column in order to see if there were some miss-labeled data types and if there were missing values. Since there were no missing values detected, it was not necessary to do any missing value imputation. (Outputs not show in order to save space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there were two data types, int and float, all integers where converted to float in order to have only one data type\n",
    "df1[['number_of_elements', 'range_atomic_radius', 'range_Valence']] = df1[['number_of_elements', 'range_atomic_radius', 'range_Valence']].astype(float)\n",
    "\n",
    "# Temperature sepparation\n",
    "def critical_temp_sep(x):\n",
    "    if x < 1.0:\n",
    "        return 'VeryLow'\n",
    "    elif x >= 1.0 and x < 5.0:\n",
    "        return 'Low'\n",
    "    elif x >= 5.0 and x < 20.0:\n",
    "        return 'Medium'\n",
    "    elif x >= 20.0 and x < 100.0:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'VeryHigh'\n",
    "    \n",
    "df1['critical_temp classes'] = df1['critical_temp'].apply(critical_temp_sep)\n",
    "\n",
    "# With the critical_temp classes created, we can remove our classification and regression labels from our X dataset and store them in two separate datasets, y_clf and y_reg, for the classification and regression tasks, respectively. \n",
    "y_clf = df1['critical_temp classes']\n",
    "y_reg = df1['critical_temp']\n",
    "X = df1.drop(columns=['critical_temp', 'critical_temp classes'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the removal of the classification and regression labels, we can now split our data into train and test datasets.For the classification data, in order to account for unbalanced classes, the performed split was done with stratification to guarantee an even class distribution in both train and test sets. This is not a necessary step for the split of the regression data. For both cases, the data was split with a 67/33 ratio (67% for training and 33% for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (14246, 81)\n",
      "Test:  (7017, 81)\n"
     ]
    }
   ],
   "source": [
    "# Data split\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y_clf, \n",
    "                     test_size=0.33,\n",
    "                     stratify=y_clf,\n",
    "                     random_state=1)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_reg, \n",
    "                    test_size=0.33,\n",
    "                    random_state=1)\n",
    "\n",
    "print('Train: ', X_train_clf.shape)\n",
    "print('Test: ', X_test_clf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the datasets divided into training and testing, we next proceeded to scale the datasets. This step needs to be done after the train and test split to avoid that the data present on the test dataset influences the training data. This problem is solved by first splitting the training and test sets and then fitting the scaler with the training set. The test set will be transformed with the fit done with the training set.\n",
    "The scaling itself is an important process since there are features in different scales that would influence the algorithms. The standardization consists of bringing the features into a mean of zero and a standard variation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of training data for classification\n",
    "std_scaler = StandardScaler()\n",
    "X_train_clf = std_scaler.fit_transform(X_train_clf)\n",
    "X_train_clf = pd.DataFrame(X_train_clf, columns = X.columns)\n",
    "#X_train_clf\n",
    "\n",
    "# Scaling of training data for linear regression\n",
    "std_scaler = StandardScaler()\n",
    "X_train_reg = std_scaler.fit_transform(X_train_reg)\n",
    "X_train_reg = pd.DataFrame(X_train_reg, columns = X.columns)\n",
    "#X_train_reg\n",
    "\n",
    "# Transformation of testing data ????????????????????\n",
    "X_test_reg = std_scaler.transform(X_test_reg)\n",
    "X_test_reg = pd.DataFrame(X_test_reg, columns = X.columns)\n",
    "#X_test_reg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRINCIPAL COMPONENT ANALYSIS\n",
    "\n",
    "Given the high number of features present in the data, a feature selection was necessary. This was achieved with Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_train_clf)\n",
    "\n",
    "pca = PCA(n_components=0.90) # our threshold ??? Did we select this threshold? I assume this is to find the nr of of components that represent 90% of de variation?\n",
    "pca.fit_transform(X_train_clf)\n",
    "\n",
    "pca = PCA(n_components = 12)\n",
    "X_train_clf = pd.DataFrame(pca.fit_transform(X_train_clf))\n",
    "#X_train_clf\n",
    "\n",
    "X_test_clf = pd.DataFrame(pca.transform(X_test_clf))\n",
    "#X_test_clf\n",
    "\n",
    "pca = PCA(n_components = 12)\n",
    "X_train_reg = pd.DataFrame(pca.fit_transform(X_train_reg))\n",
    "#X_train_reg\n",
    "\n",
    "X_test_reg = pd.DataFrame(pca.transform(X_test_reg))\n",
    "#X_test_reg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR MODEL OF THE PCA \n",
    "\n",
    "By using a PCA, we can select a subset of principal components that capture most of the variation in the data, and use them as predictors in the regression model. This can help us avoid overfitting, multicollinearity, and noise issues that may arise from using too many or irrelevant features. Below is the linear regression of the projected data followed by a linear model of the full dataset for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Linear regression of the PCA projection '''\n",
    "\n",
    "# associate data with the variables here\n",
    "X = Superconduct_X_train ##### Replace here with train\n",
    "y = Superconduct_y_train ##### Replace here with train\n",
    "\n",
    "# Create and fit a linear regression model\n",
    "model = linear_model.LinearRegression() # Create linear regression object\n",
    "model.fit(Superconduct_X_train, Superconduct_y_train) # Train the model using the training sets\n",
    "\n",
    "# Make predictions using the testing set\n",
    "Superconduct_y_pred = model.predict(Superconduct_X_test) ##### Replace here with test\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"Coefficients:\", model.coef_) # print the coefficients\n",
    "print(\"Intercept:\", model.intercept_) # print the intercept\n",
    "print(\"Mean squared error:\", mean_squared_error(y, model.predict(X))) # print the MSE on training data\n",
    "print(\"R2 score:\", r2_score(y, model.predict(X))) # print the R2 score on training data\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(Superconduct_X_test, Superconduct_y_test, color=\"black\")\n",
    "plt.plot(Superconduct_X_test, Superconduct_y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Linear regression of the full dataset '''\n",
    "\n",
    "\n",
    "# associate data with the variables here\n",
    "X = Superconduct_X ##### Replace here with full clean data\n",
    "y = Superconduct_y ##### Replace here with full clean data\n",
    "\n",
    "# Create and fit a linear regression model\n",
    "model = linear_model.LinearRegression() # Create linear regression object\n",
    "model.fit(X, y) # Train the model using the training sets \n",
    "\n",
    "# Make predictions using the testing set\n",
    "Superconduct_y_pred = model.predict(Superconduct_X_test) ##### Replace here with test  \n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"Coefficients:\", model.coef_) # print the coefficients\n",
    "print(\"Intercept:\", model.intercept_) # print the intercept\n",
    "print(\"Mean squared error:\", mean_squared_error(y, model.predict(X))) # print the MSE on training data\n",
    "print(\"R2 score:\", r2_score(y, model.predict(X))) # print the R2 score on training data\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(Superconduct_X_test, Superconduct_y_test, color=\"black\")  \n",
    "plt.plot(Superconduct_X_test, Superconduct_y_pred, color=\"blue\", linewidth=3) \n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCUSSION**\n",
    "\n",
    "\n",
    "bla bla, I wish I knew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
